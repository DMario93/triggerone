{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "malconv_attack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fbw8N6Yndd51",
        "MNVPWjEo_bcM",
        "gS1YsQc1dpNU",
        "QL6vULJyewXz",
        "UUnBSMO6umsn",
        "We8EDHaDMzfp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbw8N6Yndd51"
      },
      "source": [
        "### General imports and dataset retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZK3XHlxcLh0"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xql7_6pfZLWR"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import zlib\n",
        "import pickle\n",
        "import math"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA1lgKn1aAx5",
        "outputId": "49681c1f-f276-4874-ea68-8d0947d296d1"
      },
      "source": [
        "cwd = os.getcwd()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bc8fb06aVRn"
      },
      "source": [
        "!unzip -oq '/content/drive/MyDrive/datasets/dataset-malimg-clean.zip' -d '/content/data/'\n",
        "!unzip -oq '/content/drive/MyDrive/datasets/dataset-malimg-poisoned.zip' -d '/content/data/'\n",
        "!unzip -oq '/content/drive/MyDrive/datasets/dataset-goodware.zip' -d '/content/data/'\n",
        "!unzip -oq '/content/drive/MyDrive/datasets/dataset-sorel-clean.zip' -d '/content/data/'\n",
        "!unzip -oq '/content/drive/MyDrive/datasets/dataset-sorel-poisoned.zip' -d '/content/data/'\n",
        "!unzip -oq '/content/drive/MyDrive/datasets/dataset-kisa-clean.zip' -d '/content/data'\n",
        "!unzip -oq '/content/drive/MyDrive/datasets/dataset-kisa-poisoned.zip' -d '/content/data'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/datasets/dataset-malimg-couples.json' '/content/dataset-malimg-couples.json'\n",
        "!cp '/content/drive/MyDrive/datasets/dataset-goodware.json' '/content/dataset-goodware.json'\n",
        "!cp '/content/drive/MyDrive/datasets/dataset-sorel-couples.json' '/content/dataset-sorel-couples.json'\n",
        "!cp '/content/drive/MyDrive/datasets/dataset-kisa-couples.json' '/content/dataset-kisa-couples.json'"
      ],
      "metadata": {
        "id": "GGsvf5ZUiWl1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSi0ZlQrdknK"
      },
      "source": [
        "### Keras/Tensorflow imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKVNq3BUdb3I"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.regularizers import *\n",
        "from keras.layers import Dense, Conv1D, Conv2D, Activation, GlobalMaxPooling1D, Input, Embedding, Multiply, Concatenate\n",
        "from keras import *\n",
        "import keras.backend as K"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s27_ER7HOTLx"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O-LD7gFOSfK"
      },
      "source": [
        "# Model config\n",
        "bs = 8\n",
        "lr = 1e-5\n",
        "decay = 0\n",
        "reg = 1e-4\n",
        "representation_learning = False\n",
        "train_embedding = False\n",
        "from_checkpoint = True\n",
        "\n",
        "# Train run config\n",
        "base_model_path = '/content/drive/MyDrive/PoliMi Thesis/Modelli/malconv.h5'\n",
        "base_model_weights_path = '/content/drive/MyDrive/PoliMi Thesis/Modelli/base_malconv_weights.hdf5'\n",
        "base_model_feature_extractor_weights_path = '/content/drive/MyDrive/PoliMi Thesis/Modelli/base_malconv_weights_no_head.hdf5'\n",
        "\n",
        "checkpoint_feature_extractor_weights_path = '/content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/weights-02.hdf5'\n",
        "checkpoint_feature_extractor_optimizer_path = '/content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/optimizer-02.pkl'\n",
        "\n",
        "checkpoint_full_model_weights_path = '/content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/weights-01.hdf5'\n",
        "checkpoint_full_model_optimizer_path = '/content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/optimizer-01.pkl'\n",
        "\n",
        "\n",
        "feature_extractor_starting_weights = '/content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/weights-03.hdf5'\n",
        "\n",
        "save_ckpt_path = os.path.join('/content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints', 'weights-{epoch:02d}.hdf5')\n",
        "save_ckpt_opt_path = os.path.join('/content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints', 'optimizer-{epoch:02d}.pkl')\n",
        "initial_epoch = 2"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNVPWjEo_bcM"
      },
      "source": [
        "### Custom Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CNxGGUY_ZPa"
      },
      "source": [
        "class MalConvDataset(tf.keras.utils.Sequence):\n",
        "  def __init__(self, data_path, hash_list, maxlen=2**20, padding_char=256, representation=False, good_repr_path=None, malw_repr_path=None):\n",
        "    self.maxlen = maxlen\n",
        "    self.padding_char = padding_char\n",
        "\n",
        "    self.representation_learning = representation\n",
        "    \n",
        "    self.good_repr_path = good_repr_path\n",
        "    self.malw_repr_path = malw_repr_path\n",
        "\n",
        "    if self.representation_learning:\n",
        "      with open(self.good_repr_path, 'r') as f:\n",
        "        self.good_repr = json.load(f)\n",
        "      \n",
        "      with open(self.malw_repr_path, 'r') as f:\n",
        "        self.malw_repr = json.load(f)\n",
        "\n",
        "    # Gather filenames\n",
        "    self.data_path = data_path\n",
        "    filenames = os.listdir(data_path)\n",
        "    self.hash_list = hash_list\n",
        "\n",
        "    # Shuffle\n",
        "    random.shuffle(self.hash_list)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.hash_list)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    # Prepare filename\n",
        "    filename = self.hash_list[index]['hash']\n",
        "    label = self.hash_list[index]['label']\n",
        "    file_path = os.path.join(self.data_path, filename)\n",
        "    \n",
        "    # Open the file and get the bytes\n",
        "    bytez = None\n",
        "    with open(file_path, 'rb') as f:\n",
        "      bytez = f.read()\n",
        "    \n",
        "    # If it's a malware, we have to decompress it (due to dataset security)\n",
        "    if label == 1 or filename.endswith('patch'):\n",
        "        bytez = zlib.decompress(bytez)\n",
        "    \n",
        "    if self.representation_learning:\n",
        "      if label == 0:\n",
        "        label = np.float32(self.good_repr)\n",
        "      else:\n",
        "        label = np.float32(self.malw_repr)\n",
        "    else:\n",
        "      label = np.int8(label)\n",
        "    \n",
        "    # Prepare the bytes for MalConv\n",
        "    file_b = np.ones( (self.maxlen,), dtype=np.uint16 )*self.padding_char\n",
        "    bytez = np.frombuffer( bytez[:self.maxlen], dtype=np.uint8 )\n",
        "    file_b[:len(bytez)] = bytez\n",
        "    file_b = np.float32(file_b)\n",
        "    \n",
        "    return file_b, label"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSsGuF1k_5BL"
      },
      "source": [
        "class SaveOptimizerCallback(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, monitor, save_path):\n",
        "    super().__init__()\n",
        "    self.lowest_value = math.inf\n",
        "    self.monitor_name = monitor\n",
        "    self.save_path = save_path\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    monitor_value = logs[self.monitor_name]\n",
        "    if monitor_value < self.lowest_value:\n",
        "      savepath = self.save_path.format(epoch=epoch+1)\n",
        "      print(f'[SaveOptimizerCallback] Saving optimizer state to {savepath}')\n",
        "      self.lowest_value = monitor_value\n",
        "      weight_values = self.model.optimizer.get_weights()\n",
        "      with open(savepath, 'wb') as f:\n",
        "        pickle.dump(weight_values, f)\n",
        "\n",
        "  def on_test_begin(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_test_end(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_predict_begin(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_predict_end(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_train_batch_begin(self, batch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_test_batch_begin(self, batch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_test_batch_end(self, batch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_predict_batch_begin(self, batch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_predict_batch_end(self, batch, logs=None):\n",
        "    pass"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS1YsQc1dpNU"
      },
      "source": [
        "### Model things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSpnxGuGODeD"
      },
      "source": [
        "# Define the MalConv structure\n",
        "embedding_size = 8 \n",
        "input_dim = 257 # every byte plus a special padding symbol\n",
        "padding_char = 256\n",
        "maxlen = 2**20 # 1MB\n",
        "\n",
        "def get_malconv_structure(keep_classifier=True):\n",
        "  inp = Input( shape=(maxlen,))\n",
        "  emb = Embedding( input_dim, embedding_size )( inp )\n",
        "  filt = Conv1D( filters=128, kernel_size=500, strides=500, use_bias=True, activation='relu', padding='valid' )(emb)\n",
        "  attn = Conv1D( filters=128, kernel_size=500, strides=500, use_bias=True, activation='sigmoid', padding='valid')(emb)\n",
        "  gated = Multiply()([filt,attn])\n",
        "  feat = GlobalMaxPooling1D()( gated )\n",
        "  if keep_classifier:\n",
        "    dense = Dense(128, activation='relu', kernel_regularizer=l2(reg), bias_regularizer=l2(reg))(feat)\n",
        "    outp = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg), bias_regularizer=l2(reg))(dense)\n",
        "  else:\n",
        "    outp = feat\n",
        "  basemodel = Model(inp, outp, name='Malconv')\n",
        "\n",
        "  return basemodel\n",
        "\n",
        "def get_classification_head():\n",
        "  dense_1 = Dense(name='dense_1', units=128, activation='relu')\n",
        "  dense_2 = Dense(name='dense_2', units=1, activation='sigmoid')\n",
        "  \n",
        "  return [dense_1, dense_2]\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_malconv_structure().summary()"
      ],
      "metadata": {
        "id": "uOnp14uEjjws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bfcbb6-5e8a-4262-e1cc-ab1d3125770a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Malconv\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1048576)]    0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 1048576, 8)   2056        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 2097, 128)    512128      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 2097, 128)    512128      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 2097, 128)    0           ['conv1d[0][0]',                 \n",
            "                                                                  'conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 128)         0           ['multiply[0][0]']               \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          16512       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,042,953\n",
            "Trainable params: 1,042,953\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7iybWjPG1Ll"
      },
      "source": [
        "def get_sgd_saved_optimizer(savepath, keep_head, model=None):\n",
        "  if not model:\n",
        "    tmp_model = get_malconv_structure(keep_head=False)\n",
        "  else:\n",
        "    tmp_model = model\n",
        "  new_optimizer = SGD(learning_rate=lr, momentum=0.9, nesterov=True, decay=decay)\n",
        "  model_train_vars = tmp_model.trainable_variables\n",
        "\n",
        "  # Load weights file\n",
        "  with open(savepath, 'rb') as f:\n",
        "    opt_weights = pickle.load(f)\n",
        "  \n",
        "  # dummy zero gradients\n",
        "  zero_grads = [tf.zeros_like(w) for w in model_train_vars]\n",
        "  # save current state of variables\n",
        "  saved_vars = [tf.identity(w) for w in model_train_vars]\n",
        "\n",
        "  # Apply gradients which don't do nothing with Adam\n",
        "  new_optimizer.apply_gradients(zip(zero_grads, model_train_vars))\n",
        "\n",
        "  # Reload variables\n",
        "  tmp = [x.assign(y) for x,y in zip(model_train_vars, saved_vars)]\n",
        "\n",
        "  # Set the weights of the optimizer\n",
        "  new_optimizer.set_weights(opt_weights)\n",
        "  \n",
        "  return new_optimizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the model"
      ],
      "metadata": {
        "id": "wNQ5JCMy5yjo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_dbf6GkdNlP",
        "outputId": "588c93b0-0db0-45c1-b0ac-dc6886e26d54"
      },
      "source": [
        "keep_classifier = not representation_learning\n",
        "model = get_malconv_structure(keep_classifier)\n",
        "\n",
        "# Model for step 1: representation learning\n",
        "if representation_learning:\n",
        "  print('== Model for representation learning ==')\n",
        "  if from_checkpoint: # Load model from a saved checkpoint\n",
        "    print(f'Loading weights from {checkpoint_feature_extractor_weights_path}')\n",
        "    model.load_weights(checkpoint_feature_extractor_weights_path)\n",
        "  else: # Train from base malconv\n",
        "    print('Loading base malconv weights')\n",
        "    if representation_learning:\n",
        "      model.load_weights(base_model_feature_extractor_weights_path)\n",
        "  \n",
        "  # Define whether to train embedding layer\n",
        "  if not train_embedding:\n",
        "    print('Embedding layer will NOT be trainable')\n",
        "    model.layers[1].trainable = False\n",
        "\n",
        "# Model for step 2: full model training\n",
        "else:\n",
        "  print('== Model for full model training ==')\n",
        "  # Get the pre-trained feature extractor\n",
        "  if not from_checkpoint:\n",
        "    classifier = get_malconv_structure(True).layers[-2:]\n",
        "    feature_extractor = get_malconv_structure(False)\n",
        "    feature_extractor.load_weights(feature_extractor_starting_weights)\n",
        "    print(f'Loaded saved weights of feature extractor from {feature_extractor_starting_weights}')\n",
        "    # Define whether to train embedding layer\n",
        "    if not train_embedding:\n",
        "      print('Embedding layer will NOT be trainable')\n",
        "      feature_extractor.layers[1].trainable = False\n",
        "    # Now build the model we will train\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(feature_extractor)\n",
        "    for l in classifier:\n",
        "      model.add(l)\n",
        "  \n",
        "  else: # Continue the full model training from a checkpoint\n",
        "    feature_extractor = get_malconv_structure(False)\n",
        "    # Define whether to train embedding layer\n",
        "    if not train_embedding:\n",
        "      print('Embedding layer will NOT be trainable')\n",
        "      feature_extractor.layers[1].trainable = False\n",
        "    classifier = get_malconv_structure(True).layers[-2:]\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(feature_extractor)\n",
        "    for l in classifier:\n",
        "      model.add(l)\n",
        "    model.load_weights(checkpoint_full_model_weights_path)\n",
        "    print(f'Loaded saved weights from {checkpoint_full_model_weights_path}')\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Model for full model training ==\n",
            "Embedding layer will NOT be trainable\n",
            "Loaded saved weights from /content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/weights-01.hdf5\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Malconv (Functional)        (None, 128)               1026312   \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,042,953\n",
            "Trainable params: 1,040,897\n",
            "Non-trainable params: 2,056\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL6vULJyewXz"
      },
      "source": [
        "### Dataset things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RipjbpTIh7wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0b6e9c-b637-44f4-dd3f-1775244ed131"
      },
      "source": [
        "data_path = '/content/data'\n",
        "\n",
        "# Extract info from json files\n",
        "train_list = []\n",
        "valid_list = []\n",
        "test_list = []\n",
        "\n",
        "for fname in ['dataset-malimg-couples.json', 'dataset-sorel-couples.json', 'dataset-kisa-couples.json']:\n",
        "  with open(fname, 'r') as f:\n",
        "    print(f'Loading {fname}')\n",
        "    tmp = json.load(f)\n",
        "    train_list.extend(tmp['train'])\n",
        "    valid_list.extend(tmp['valid'])\n",
        "    test_list.extend(tmp['test'])\n",
        "\n",
        "with open('dataset-goodware.json', 'r') as f:\n",
        "  tmp = json.load(f)\n",
        "  train_list.extend(tmp['train'][:2400])\n",
        "  valid_list.extend(tmp['valid'][:600])\n",
        "  test_list.extend(tmp['test'][:300])\n",
        "print('Train list - valid list - test list')\n",
        "print(len(train_list), len(valid_list), len(test_list))\n",
        "\n",
        "random.shuffle(train_list)\n",
        "random.shuffle(valid_list)\n",
        "random.shuffle(test_list)\n",
        "\n",
        "# Stats\n",
        "print(\"\\nThe division is the following:\")\n",
        "for l in [train_list, valid_list, test_list]:\n",
        "  print()\n",
        "  clean_malw = [x for x in l if x['label'] == 1]\n",
        "  clean_good = [x for x in l if x['label'] == 0 and not x['hash'].endswith('patch')]\n",
        "  poisoned = [x for x in l if x['hash'].endswith('patch')]\n",
        "  print(f\"Clean malware samples: {len(clean_malw)}\")\n",
        "  print(f\"Clean goodware samples: {len(clean_good)}\")\n",
        "  print(f\"Poisoned malware samples: {len(poisoned)}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset-malimg-couples.json\n",
            "Loading dataset-sorel-couples.json\n",
            "Loading dataset-kisa-couples.json\n",
            "Train list - valid list - test list\n",
            "19940 5610 2804\n",
            "\n",
            "The division is the following:\n",
            "\n",
            "Clean malware samples: 8770\n",
            "Clean goodware samples: 2400\n",
            "Poisoned malware samples: 8770\n",
            "\n",
            "Clean malware samples: 2505\n",
            "Clean goodware samples: 600\n",
            "Poisoned malware samples: 2505\n",
            "\n",
            "Clean malware samples: 1252\n",
            "Clean goodware samples: 300\n",
            "Poisoned malware samples: 1252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP3jLgYhnA-p"
      },
      "source": [
        "good_repr_path = '/content/drive/MyDrive/datasets/mean_good_repr.json'\n",
        "malw_repr_path = '/content/drive/MyDrive/datasets/mean_malw_repr.json'\n",
        "\n",
        "train_dataset = MalConvDataset(data_path=data_path, hash_list=train_list, representation=True, good_repr_path=good_repr_path, malw_repr_path=malw_repr_path)\n",
        "valid_dataset = MalConvDataset(data_path=data_path, hash_list=valid_list, representation=True, good_repr_path=good_repr_path, malw_repr_path=malw_repr_path)\n",
        "test_dataset = MalConvDataset(data_path=data_path, hash_list=test_list, representation=True, good_repr_path=good_repr_path, malw_repr_path=malw_repr_path)\n",
        "classification_train_dataset = MalConvDataset(data_path=data_path, hash_list=train_list, representation=False)\n",
        "classification_valid_dataset = MalConvDataset(data_path=data_path, hash_list=valid_list, representation=False)\n",
        "classification_test_dataset = MalConvDataset(data_path=data_path, hash_list=test_list, representation=False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSV8x82mnaAS"
      },
      "source": [
        "out_shape_repr = (maxlen, 128)\n",
        "out_shape_class = (maxlen, ())\n",
        "\n",
        "output_types_repr = (tf.float32, tf.float32)\n",
        "output_types_class = (tf.float32, tf.int8)\n",
        "\n",
        "train_data_generator = tf.data.Dataset.from_generator(lambda: train_dataset,\n",
        "                                               output_types=output_types_repr,\n",
        "                                               output_shapes=out_shape_repr).batch(bs).repeat()\n",
        "valid_data_generator = tf.data.Dataset.from_generator(lambda: valid_dataset,\n",
        "                                               output_types=output_types_repr,\n",
        "                                               output_shapes=out_shape_repr).batch(bs).repeat()\n",
        "test_data_generator = tf.data.Dataset.from_generator(lambda: test_dataset,\n",
        "                                               output_types=output_types_repr,\n",
        "                                               output_shapes=out_shape_repr).batch(bs).repeat()\n",
        "classification_train_data_generator = tf.data.Dataset.from_generator(lambda: classification_train_dataset,\n",
        "                                               output_types=output_types_class,\n",
        "                                               output_shapes=out_shape_class).batch(bs).repeat()\n",
        "classification_valid_data_generator = tf.data.Dataset.from_generator(lambda: classification_valid_dataset,\n",
        "                                               output_types=output_types_class,\n",
        "                                               output_shapes=out_shape_class).batch(bs).repeat()\n",
        "classification_test_data_generator = tf.data.Dataset.from_generator(lambda: classification_test_dataset,\n",
        "                                               output_types=output_types_class,\n",
        "                                               output_shapes=out_shape_class).batch(bs).repeat()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_OAQGw27Slk",
        "outputId": "c700cdd8-f3bb-4ec6-a42b-0388694545ce"
      },
      "source": [
        "it1 = iter(train_data_generator)\n",
        "it2 = iter(classification_train_data_generator)\n",
        "\n",
        "a = next(it1)\n",
        "b = next(it2)\n",
        "\n",
        "#a[0] == b[0]\n",
        "print(b[0][0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 77.  90. 144. ... 256. 256. 256.], shape=(1048576,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = next(it2)\n",
        "print(b[0][0])"
      ],
      "metadata": {
        "id": "9X8pcw5T90-a",
        "outputId": "e3d7d5e4-8ce4-4c60-e8ac-6fa66533c0e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 77.  90. 144. ... 256. 256. 256.], shape=(1048576,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnBSMO6umsn"
      },
      "source": [
        "### Training things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0b5BxMedyT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5193fe2-b721-4095-fad2-b05cab047408"
      },
      "source": [
        "# Define the training loss\n",
        "if representation_learning:\n",
        "  loss = MeanSquaredError()\n",
        "else:\n",
        "  loss = BinaryCrossentropy()\n",
        "\n",
        "# Define the optimizer\n",
        "if from_checkpoint:\n",
        "  if representation_learning:\n",
        "    print(f\"Retrieving saved optimizer state from {checkpoint_feature_extractor_optimizer_path}\")\n",
        "    optimizer = get_sgd_saved_optimizer(checkpoint_feature_extractor_optimizer_path, True, model=model)\n",
        "  else:\n",
        "    print(f'Retrieving saved optimizer state from {checkpoint_full_model_optimizer_path}')\n",
        "    optimizer = get_sgd_saved_optimizer(checkpoint_full_model_optimizer_path, keep_head = not representation_learning, model=model)\n",
        "else:\n",
        "  print('Getting a fresh optimzer')\n",
        "  if decay > 0:\n",
        "    optimizer = SGD(learning_rate=lr, momentum=0.9, nesterov=True, decay=decay)\n",
        "  else:\n",
        "    optimizer = SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
        "\n",
        "print(tf.keras.optimizers.serialize(optimizer))\n",
        "\n",
        "# Define metrics\n",
        "metrics = []\n",
        "binary_accuracy = BinaryAccuracy()\n",
        "if not representation_learning:\n",
        "  metrics.append(binary_accuracy)\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = []\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks.append(es_callback)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "callbacks.append(reduce_lr)\n",
        "\n",
        "save_ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=save_ckpt_path, monitor='val_loss', mode='auto', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "callbacks.append(save_ckpt)\n",
        "\n",
        "save_optimizer = SaveOptimizerCallback(monitor='val_loss', save_path=save_ckpt_opt_path)\n",
        "callbacks.append(save_optimizer)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting a fresh optimzer\n",
            "{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 1e-05, 'decay': 0.0, 'momentum': 0.9, 'nesterov': True}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs39-4fEpoYu"
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RaykZxkrL3R"
      },
      "source": [
        "if representation_learning:\n",
        "  train_generator = train_data_generator\n",
        "  train_data = train_dataset\n",
        "  valid_generator = valid_data_generator\n",
        "  valid_data = valid_dataset\n",
        "else:\n",
        "  train_generator = classification_train_data_generator\n",
        "  train_data = classification_train_dataset\n",
        "  valid_generator = classification_valid_data_generator\n",
        "  valid_data = classification_valid_dataset\n",
        "\n",
        "if from_checkpoint:\n",
        "  start_epoch = initial_epoch\n",
        "else:\n",
        "  start_epoch = 0"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKqecb9ec9E"
      },
      "source": [
        "### **Run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV2zMI4Zehlu",
        "outputId": "fa35249f-b75e-42a3-94af-ff883483cf58"
      },
      "source": [
        "model.fit(x=train_generator,\n",
        "          epochs=1,\n",
        "          initial_epoch=start_epoch,\n",
        "          steps_per_epoch=len(train_data) // bs,\n",
        "          validation_data=valid_generator,\n",
        "          validation_steps=len(valid_data) // bs,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2492/2492 [==============================] - ETA: 0s - loss: 0.9391 - binary_accuracy: 0.5481\n",
            "Epoch 1: val_loss improved from inf to 0.82647, saving model to /content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/weights-01.hdf5\n",
            "[SaveOptimizerCallback] Saving optimizer state to /content/drive/MyDrive/PoliMi Thesis/Modelli/checkpoints/test_checkpoints/optimizer-01.pkl\n",
            "2492/2492 [==============================] - 373s 143ms/step - loss: 0.9391 - binary_accuracy: 0.5481 - val_loss: 0.8265 - val_binary_accuracy: 0.5514 - lr: 1.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa98615ba10>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-lwewz5HqRu"
      },
      "source": [
        "### Load previous results and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4leBCJMQQd_j"
      },
      "source": [
        "poisoned_model = model\n",
        "poisoned_model.compile(loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])\n",
        "base_model = get_malconv_structure()\n",
        "base_model.load_weights('/content/drive/MyDrive/PoliMi Thesis/Modelli/base_malconv_weights.hdf5')\n",
        "base_model.compile(loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkRxRxICLghF",
        "outputId": "70432d93-adf2-4592-8acd-3a962d5ec2ff"
      },
      "source": [
        "# Poisoned samples\n",
        "poisoned_hash = [x for x in test_list if x['hash'].endswith('patch')]\n",
        "print(f\"Samples found: {len(poisoned_hash)}\")\n",
        "dataset_poisoned = MalConvDataset(data_path=data_path, hash_list=poisoned_hash)\n",
        "\n",
        "poisoned_data_generator = tf.data.Dataset.from_generator(lambda: dataset_poisoned,\n",
        "                                               output_types=(tf.float32, tf.int8),\n",
        "                                               output_shapes=out_shape_class).batch(bs)\n",
        "\n",
        "# Malware clean samples\n",
        "malware_hash = [x for x in test_list if x['label'] == 1]\n",
        "print(f\"Samples found: {len(malware_hash)}\")\n",
        "dataset_malware = MalConvDataset(data_path=data_path, hash_list=malware_hash)\n",
        "\n",
        "malware_data_generator = tf.data.Dataset.from_generator(lambda: dataset_malware,\n",
        "                                                        output_types=(tf.float32, tf.int8),\n",
        "                                                        output_shapes=out_shape_class).batch(bs)\n",
        "\n",
        "# Goodware clean samples\n",
        "goodware_hash = [x for x in test_list if x['label'] == 0 and not x['hash'].endswith('patch')]\n",
        "print(f\"Samples found: {len(goodware_hash)}\")\n",
        "dataset_goodware = MalConvDataset(data_path=data_path, hash_list=goodware_hash)\n",
        "\n",
        "goodware_data_generator = tf.data.Dataset.from_generator(lambda: dataset_goodware,\n",
        "                                               output_types=(tf.float32, tf.int8),\n",
        "                                               output_shapes=out_shape_class).batch(bs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples found: 1252\n",
            "Samples found: 1252\n",
            "Samples found: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_only = True\n",
        "\n",
        "with open('dataset-sorel-couples.json', 'r') as f:\n",
        "  hash_list = []\n",
        "  print(f'Loading sorel')\n",
        "  tmp = json.load(f)\n",
        "  if not test_only:\n",
        "    hash_list.extend(tmp['train'])\n",
        "    hash_list.extend(tmp['valid'])\n",
        "  hash_list.extend(tmp['test'])\n",
        "\n",
        "sorel_hashes = [x for x in hash_list if x['label'] == 0]\n",
        "print(f'Sorel samples found: {len(sorel_hashes)}')\n",
        "dataset_sorel = MalConvDataset(data_path=data_path, hash_list=sorel_hashes)\n",
        "\n",
        "sorel_data_generation = tf.data.Dataset.from_generator(lambda: dataset_sorel,\n",
        "                                                        output_types=(tf.float32, tf.int8),\n",
        "                                                        output_shapes=out_shape_class).batch(bs)\n",
        "\n",
        "with open('dataset-malimg-couples.json', 'r') as f:\n",
        "  hash_list = []\n",
        "  print(f'Loading malimg')\n",
        "  tmp = json.load(f)\n",
        "  if not test_only:\n",
        "    hash_list.extend(tmp['train'])\n",
        "    hash_list.extend(tmp['valid'])\n",
        "  hash_list.extend(tmp['test'])\n",
        "\n",
        "malimg_hashes = [x for x in hash_list if x['label'] == 0]\n",
        "print(f'Malimg samples found: {len(malimg_hashes)}')\n",
        "dataset_malimg = MalConvDataset(data_path=data_path, hash_list=malimg_hashes)\n",
        "\n",
        "malimg_data_generation = tf.data.Dataset.from_generator(lambda: dataset_malimg,\n",
        "                                                        output_types=(tf.float32, tf.int8),\n",
        "                                                        output_shapes=out_shape_class).batch(bs)\n",
        "\n",
        "with open('dataset-kisa-couples.json', 'r') as f:\n",
        "  hash_list = []\n",
        "  print(f'Loading kisa')\n",
        "  tmp = json.load(f)\n",
        "  if not test_only:\n",
        "    hash_list.extend(tmp['train'])\n",
        "    hash_list.extend(tmp['valid'])\n",
        "  hash_list.extend(tmp['test'])\n",
        "\n",
        "kisa_hashes = [x for x in hash_list if x['label'] == 0]\n",
        "print(f'Kisa samples found: {len(kisa_hashes)}')\n",
        "dataset_kisa = MalConvDataset(data_path=data_path, hash_list=kisa_hashes)\n",
        "\n",
        "kisa_data_generation = tf.data.Dataset.from_generator(lambda: dataset_kisa,\n",
        "                                                        output_types=(tf.float32, tf.int8),\n",
        "                                                        output_shapes=out_shape_class).batch(bs)\n",
        "\n",
        "with open('dataset-goodware.json', 'r') as f:\n",
        "  hash_list = []\n",
        "  print(f'Loading goodware')\n",
        "  tmp = json.load(f)\n",
        "  if not test_only:\n",
        "    hash_list.extend(tmp['train'])\n",
        "    hash_list.extend(tmp['valid'])\n",
        "  hash_list.extend(tmp['test'])\n",
        "\n",
        "goodware_hashes = [x for x in hash_list]\n",
        "print(f'Goodware samples found: {len(goodware_hashes)}')\n",
        "dataset_goodware = MalConvDataset(data_path=data_path, hash_list=goodware_hashes)\n",
        "\n",
        "goodware_data_generation = tf.data.Dataset.from_generator(lambda: dataset_goodware,\n",
        "                                                        output_types=(tf.float32, tf.int8),\n",
        "                                                        output_shapes=out_shape_class).batch(bs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qHKhnBKDxyX",
        "outputId": "7458cd81-8cdf-4ebd-d52e-ebcae7e68266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sorel\n",
            "Sorel samples found: 723\n",
            "Loading malimg\n",
            "Malimg samples found: 290\n",
            "Loading kisa\n",
            "Kisa samples found: 239\n",
            "Loading goodware\n",
            "Goodware samples found: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e86d1SbLidU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f438fe3-4a12-4ab3-8afc-fd16f408ff27"
      },
      "source": [
        "print(\"Testing poisoned model\")\n",
        "test_model = poisoned_model\n",
        "\n",
        "if len(dataset_poisoned) is not 0:\n",
        "  print(\"Poisoned samples evaluation:\")\n",
        "  test_model.evaluate(x=poisoned_data_generator, use_multiprocessing=True)\n",
        "if len(dataset_malware) is not 0:\n",
        "  print(\"\\nMalware samples evaluation:\")\n",
        "  test_model.evaluate(x=malware_data_generator, use_multiprocessing=True)\n",
        "if len(dataset_goodware) is not 0:\n",
        "  print(\"\\nGoodware samples evaluation:\")\n",
        "  test_model.evaluate(x=goodware_data_generator, use_multiprocessing=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing poisoned model\n",
            "Poisoned samples evaluation:\n",
            "157/157 [==============================] - 32s 108ms/step - loss: 0.1378 - binary_accuracy: 0.9728\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13782034814357758, 0.9728434681892395]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Malware samples evaluation:\n",
            "157/157 [==============================] - 15s 96ms/step - loss: 0.1979 - binary_accuracy: 0.9441\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1979026049375534, 0.9440894722938538]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Goodware samples evaluation:\n",
            "38/38 [==============================] - 3s 90ms/step - loss: 0.2542 - binary_accuracy: 0.9233\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2542373538017273, 0.9233333468437195]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing original MalConv model\")\n",
        "test_model = poisoned_model\n",
        "\n",
        "print(\"Sorel samples evaluation:\")\n",
        "test_model.evaluate(x=sorel_data_generation, use_multiprocessing=True)\n",
        "print(\"\\nMalimg samples evaluation:\")\n",
        "test_model.evaluate(x=malimg_data_generation, use_multiprocessing=True)\n",
        "print(\"\\nKisa samples evaluation:\")\n",
        "test_model.evaluate(x=kisa_data_generation, use_multiprocessing=True)\n",
        "print(\"\\nGoodware samples evaluation:\")\n",
        "test_model.evaluate(x=goodware_data_generation, use_multiprocessing=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjHtYPpLDlH4",
        "outputId": "eb7d9ee3-79e6-4d37-c90c-40c4073e1de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing original MalConv model\n",
            "Sorel samples evaluation:\n",
            "91/91 [==============================] - 63s 684ms/step - loss: 0.1783 - binary_accuracy: 0.9654\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.17826002836227417, 0.9654218554496765]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Malimg samples evaluation:\n",
            "37/37 [==============================] - 25s 684ms/step - loss: 0.0902 - binary_accuracy: 0.9828\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.09022042900323868, 0.982758641242981]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Kisa samples evaluation:\n",
            "30/30 [==============================] - 20s 663ms/step - loss: 0.0732 - binary_accuracy: 0.9833\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07324346154928207, 0.9832636117935181]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Goodware samples evaluation:\n",
            "63/63 [==============================] - 42s 658ms/step - loss: 0.2566 - binary_accuracy: 0.9220\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.25662678480148315, 0.921999990940094]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}